{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn.manifold\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import pandas\n",
    "import pickle\n",
    "import os\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = '../../histnorm/datasets/historical/spanish/spanish-ps<n>.dev.txt'\n",
    "ENCODING = 'utf-8'\n",
    "CORPUS_NAME = 'spanish-ps'\n",
    "FILTER = ('\"', \"'\", '#', '.', ',', '(', ')', ';', 'â€”', '/')\n",
    "\n",
    "tokens_raw = []\n",
    "\n",
    "# Loading input file, which has the original and modernised token in each line separated by a \\t\n",
    "for n in range(16,20):\n",
    "    inputfile = INPUT.replace('<n>', str(n))\n",
    "    with open(inputfile, 'r', encoding=ENCODING) as infile:\n",
    "        tokens_raw += [line.strip().split('\\t') for line in infile]\n",
    "\n",
    "# Filter out lines with control characters\n",
    "tokens = [token for token in tokens_raw if len(token)>1 and not token[0].startswith(FILTER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the original and modernised tokens and types\n",
    "tokens_original = [token[0].lower() for token in tokens if len(token) > 1]\n",
    "tokens_modernised = [token[1].lower() for token in tokens if len(token) > 1]\n",
    "\n",
    "types_original = list(set(tokens_original))\n",
    "types_modernised = list(set(tokens_modernised))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttr(types, tokens):\n",
    "    \"\"\"\n",
    "    Calculating Type-Token Ration\n",
    "    \"\"\"\n",
    "    return len(types)/len(tokens)\n",
    "\n",
    "assert ttr([0]*5, [0]*10)  == 0.5\n",
    "assert ttr([0]*10, [0]*10) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_original_count = collections.Counter(tokens_original)\n",
    "tokens_modernised_count = collections.Counter(tokens_modernised)\n",
    "\n",
    "hapax_original_count = len([val for val in tokens_original_count.values() if val == 1])\n",
    "hapax_modernised_count = len([val for val in tokens_modernised_count.values() if val == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CORPUS_NAME)\n",
    "print('Tokens Original Example: {}'.format(tokens_original[:10]))\n",
    "print('Tokens Original Count: {}'.format(len(tokens_original)))\n",
    "print('Types Original Example: {}'.format(types_original[:10]))\n",
    "print('Types Original Count: {}'.format(len(types_original)))\n",
    "print('Type/Token Ratio Original: {:2.2%}'.format(ttr(types_original, tokens_original)))\n",
    "print('Hapax Original Count: {}'.format(hapax_original_count))\n",
    "print('Tokens Most Common Original: {}'.format(str(tokens_original_count.most_common(10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CORPUS_NAME)\n",
    "print('Tokens Modernised Example: {}'.format(tokens_modernised[:10]))\n",
    "print('Tokens Modernised Count: {}'.format(len(tokens_modernised)))\n",
    "print('Types Modernised Example: {}'.format(types_modernised[:10]))\n",
    "print('Types Modernised Count: {}'.format(len(types_modernised)))\n",
    "print('Type/Token Modernised Original: {:2.2%}'.format(ttr(types_modernised, tokens_modernised)))\n",
    "print('Hapax Original Count: {}'.format(hapax_modernised_count))\n",
    "print('Tokens Most Common Original: {}'.format(str(tokens_modernised_count.most_common(10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Cluster\n",
    "evaluation_cluster = dict()\n",
    "for token in tokens:\n",
    "    if token[1].lower() in evaluation_cluster:\n",
    "        evaluation_cluster[token[1].lower()].append(token[0].lower())\n",
    "    else:\n",
    "        evaluation_cluster[token[1].lower()] = [token[0].lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(evaluation_cluster) == len(types_modernised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average cluster size\n",
    "sum([len(val) for val in evaluation_cluster.values()]) / len(evaluation_cluster.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variations for 10 most common tokens\n",
    "for token in tokens_modernised_count.most_common(10):\n",
    "    print('{}: {}\\n'.format(token[0], set(evaluation_cluster[token[0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(string1, string2):\n",
    "    \"\"\"\n",
    "    Levenshtein Distance between two strings\n",
    "    \"\"\"\n",
    "    if string1 == string2:\n",
    "        return 0\n",
    "\n",
    "    rows = len(string1) + 1\n",
    "    cols = len(string2) + 1\n",
    "    dist = [[0 for c in range(cols)] for r in range(rows)]\n",
    "\n",
    "    for j in range(1, rows):\n",
    "        dist[j][0] = j\n",
    "    for i in range(1, cols):\n",
    "        dist[0][i] = i\n",
    "\n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            cost = 1\n",
    "            if string1[row - 1] == string2[col - 1]:\n",
    "                cost = 0\n",
    "            dist[row][col] = min(dist[row - 1][col] + 1, dist[row][col - 1] + 1, dist[row - 1][col - 1] + cost)\n",
    "\n",
    "    return dist[row][col]\n",
    "\n",
    "assert levenshtein('foobar', 'foobar') == 0\n",
    "assert levenshtein('foobar', 'foubar') == 1\n",
    "assert levenshtein('foobar', 'fuubar') == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "similarity = levenshtein\n",
    "cache_name = CORPUS_NAME + '-pairwise-distance.pickle'\n",
    "\n",
    "if os.path.exists(cache_name):\n",
    "    print('> Using Cache')\n",
    "    # Unpacking pickled Pairwise Distances\n",
    "    with open(cache_name, 'rb' ) as pickler:    \n",
    "        cache = pickle.load(pickler)\n",
    "        types_original_pairwise_distance = cache['types_original_pairwise_distance']\n",
    "        types_modernised_pairwise_distance = cache['types_modernised_pairwise_distance']\n",
    "\n",
    "else:\n",
    "    # Calculating string distances for each type\n",
    "    types_original_reshaped = np.array(types_original).reshape(-1,1)\n",
    "    types_original_pairwise_distance = scipy.spatial.distance.pdist(types_original_reshaped, lambda x,y: similarity(str(x[0]),str(y[0])))   \n",
    "\n",
    "    types_modernised_reshaped = np.array(types_modernised).reshape(-1,1)\n",
    "    types_modernised_pairwise_distance = scipy.spatial.distance.pdist(types_modernised_reshaped, lambda x,y: similarity(str(x[0]),str(y[0])))   \n",
    "\n",
    "    print('> Writing Cache')\n",
    "    with open(cache_name, 'wb' ) as pickler:\n",
    "        data = {\n",
    "            'types_original_pairwise_distance': types_original_pairwise_distance,\n",
    "            'types_modernised_pairwise_distance': types_modernised_pairwise_distance\n",
    "        }\n",
    "        pickle.dump(data, pickler) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Transforming pairwise distances into a full similarity matrix\n",
    "original_distance_matrix = pandas.DataFrame(scipy.spatial.distance.squareform(types_original_pairwise_distance), index=types_original, columns=types_original)\n",
    "modernised_distance_matrix = pandas.DataFrame(scipy.spatial.distance.squareform(types_modernised_pairwise_distance), index=types_modernised, columns=types_modernised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Running t-SNE on similarity matrix\n",
    "\n",
    "cache_name = CORPUS_NAME + '-tsne.pickle'\n",
    "\n",
    "if os.path.exists(cache_name):\n",
    "    print('> Using Cache')\n",
    "    # Unpacking pickled Pairwise Distances\n",
    "    with open(cache_name, 'rb' ) as pickler:    \n",
    "        cache = pickle.load(pickler)\n",
    "        original_similarity_embedded = cache['original_similarity_embedded']\n",
    "        modernised_similarity_embedded = cache['modernised_similarity_embedded']\n",
    "\n",
    "else:\n",
    "    original_similarity_embedded = sklearn.manifold.TSNE(n_components=2).fit_transform(original_distance_matrix)\n",
    "    modernised_similarity_embedded = sklearn.manifold.TSNE(n_components=2).fit_transform(modernised_distance_matrix)\n",
    "    \n",
    "    print('> Writing Cache')\n",
    "    with open(cache_name, 'wb' ) as pickler:\n",
    "        data = {\n",
    "            'original_similarity_embedded': original_similarity_embedded,\n",
    "            'modernised_similarity_embedded': modernised_similarity_embedded\n",
    "        }\n",
    "        pickle.dump(data, pickler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Running UMAP on similarity matrix\n",
    "\n",
    "cache_name = CORPUS_NAME + '-umap.pickle'\n",
    "\n",
    "if os.path.exists(cache_name):\n",
    "    print('> Using Cache')\n",
    "    # Unpacking pickled Pairwise Distances\n",
    "    with open(cache_name, 'rb' ) as pickler:    \n",
    "        cache = pickle.load(pickler)\n",
    "        original_similarity_embedded_umap = cache['original_similarity_embedded_umap']\n",
    "        modernised_similarity_embedded_umap = cache['modernised_similarity_embedded_umap']\n",
    "\n",
    "else:\n",
    "    original_similarity_embedded_umap = umap.UMAP(n_components=2).fit_transform(original_distance_matrix)\n",
    "    modernised_similarity_embedded_umap = umap.UMAP(n_components=2).fit_transform(modernised_distance_matrix)\n",
    "    \n",
    "    print('> Writing Cache')\n",
    "    with open(cache_name, 'wb' ) as pickler:\n",
    "        data = {\n",
    "            'original_similarity_embedded_umap': original_similarity_embedded_umap,\n",
    "            'modernised_similarity_embedded_umap': modernised_similarity_embedded_umap\n",
    "        }\n",
    "        pickle.dump(data, pickler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tsne = pandas.DataFrame()\n",
    "original_tsne['tsne-x-original'] = original_similarity_embedded[:,0]\n",
    "original_tsne['tsne-y-original'] = original_similarity_embedded[:,1]\n",
    "\n",
    "modernised_tsne = pandas.DataFrame()\n",
    "modernised_tsne['tsne-x-modernised'] = modernised_similarity_embedded[:,0]\n",
    "modernised_tsne['tsne-y-modernised'] = modernised_similarity_embedded[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_umap = pandas.DataFrame()\n",
    "original_umap['umap-x-original'] = original_similarity_embedded_umap[:,0]\n",
    "original_umap['umap-y-original'] = original_similarity_embedded_umap[:,1]\n",
    "\n",
    "modernised_umap = pandas.DataFrame()\n",
    "modernised_umap['umap-x-modernised'] = modernised_similarity_embedded_umap[:,0]\n",
    "modernised_umap['umap-y-modernised'] = modernised_similarity_embedded_umap[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens_original_count = pandas.DataFrame.from_dict(tokens_original_count, orient='index').reset_index()\n",
    "df_tokens_original_count = df_tokens_original_count.rename(columns={'index': 'token', 0: 'occurrence'})\n",
    "df_tokens_original_count = df_tokens_original_count.sort_values(by=['occurrence'], ascending=False)\n",
    "\n",
    "df_tokens_modernised_count = pandas.DataFrame.from_dict(tokens_modernised_count, orient='index').reset_index()\n",
    "df_tokens_modernised_count = df_tokens_modernised_count.rename(columns={'index': 'token', 0: 'occurrence'})\n",
    "df_tokens_modernised_count = df_tokens_modernised_count.sort_values(by=['occurrence'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 25 \n",
    "plt.figure(figsize=(20,20))\n",
    "sns.countplot(\n",
    "    data=df_tokens_original_count,\n",
    "    order = df_tokens_original_count['occurrence'].value_counts().iloc[:TOP_N].index,\n",
    "    x='occurrence',\n",
    "    color='steelblue',\n",
    ").set_title(CORPUS_NAME + '-token-occurrences-original-top-' + str(TOP_N))\n",
    "\n",
    "plt.savefig(CORPUS_NAME + '-token-occurrences-original-top-' + str(TOP_N) +' .png', \n",
    "            facecolor='white',\n",
    "            bbox_inches='tight', \n",
    "            dpi=100,\n",
    "            pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 25 \n",
    "plt.figure(figsize=(20,20))\n",
    "sns.countplot(\n",
    "    data=df_tokens_modernised_count,\n",
    "    order = df_tokens_modernised_count['occurrence'].value_counts().iloc[:TOP_N].index,\n",
    "    x='occurrence',\n",
    "    color='steelblue',\n",
    ").set_title(CORPUS_NAME + '-token-occurrences-modernised-top-' + str(TOP_N))\n",
    "\n",
    "plt.savefig(CORPUS_NAME + '-token-occurrences-modernised-top-' + str(TOP_N) +' .png', \n",
    "            facecolor='white',\n",
    "            bbox_inches='tight', \n",
    "            dpi=100,\n",
    "            pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "sns.scatterplot(\n",
    "    x='tsne-x-original', y='tsne-y-original',\n",
    "    data=original_tsne,\n",
    "    alpha=0.5\n",
    ").set_title(CORPUS_NAME + '-tsne-original')\n",
    "\n",
    "plt.savefig('spanish-ps-tsne-original.png', \n",
    "            facecolor='white',\n",
    "            bbox_inches='tight', \n",
    "            dpi=100,\n",
    "            pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "sns.scatterplot(\n",
    "    x='tsne-x-modernised', y='tsne-y-modernised',\n",
    "    data=modernised_tsne,\n",
    "    alpha=0.5\n",
    ").set_title(CORPUS_NAME + '-tsne-modernised')\n",
    "\n",
    "plt.savefig(CORPUS_NAME + '-tsne-modernised.png', \n",
    "            facecolor='white',\n",
    "            bbox_inches='tight', \n",
    "            dpi=100,\n",
    "            pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "sns.scatterplot(\n",
    "    x='umap-x-original', y='umap-y-original',\n",
    "    data=original_umap,\n",
    "    alpha=0.5\n",
    ").set_title(CORPUS_NAME + '-umap-original')\n",
    "\n",
    "plt.savefig(CORPUS_NAME + '-umap-original.png', \n",
    "            facecolor='white',\n",
    "            bbox_inches='tight', \n",
    "            dpi=100,\n",
    "            pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "sns.scatterplot(\n",
    "    x='umap-x-modernised', y='umap-y-modernised',\n",
    "    data=modernised_umap,\n",
    "    alpha=0.5\n",
    ").set_title(CORPUS_NAME + '-umap-modernised')\n",
    "\n",
    "plt.savefig(CORPUS_NAME + '-umap-modernised.png', \n",
    "            facecolor='white',\n",
    "            bbox_inches='tight', \n",
    "            dpi=100,\n",
    "            pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Clustering the pairwise distances\n",
    "linkage_method = 'ward'\n",
    "original_clustering = scipy.cluster.hierarchy.linkage(types_original_pairwise_distance, linkage_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from json import dump\n",
    "\n",
    "labels = dict(enumerate(types_original))\n",
    "\n",
    "def add_nodes(node, parent):\n",
    "    \"\"\"\n",
    "    Recursively build tree as dict\n",
    "    \"\"\"\n",
    "    new_node = dict(node_id=node.id, children=[], distance=node.dist)\n",
    "    parent['children'].append(new_node)\n",
    "    if node.left: add_nodes(node.left, new_node)\n",
    "    if node.right: add_nodes(node.right, new_node)\n",
    "\n",
    "def add_labels(node):\n",
    "    \"\"\"\n",
    "    Recursively add labels to the tree\n",
    "    \"\"\"\n",
    "    is_leaf = len(node['children']) == 0\n",
    "\n",
    "    if is_leaf: \n",
    "        node['name'] = labels[node['node_id']]\n",
    "    else:\n",
    "        list(map(add_labels, node['children']))  \n",
    "    del node['node_id']\n",
    "\n",
    "if not os.path.exists(CORPUS_NAME + '-cluster-original.json'):\n",
    "    # Transforming Cluster into JSON Tree\n",
    "    scipy_tree = scipy.cluster.hierarchy.to_tree(original_clustering, rd=False)\n",
    "    tree = dict(name='root', children=[], distance=scipy_tree.dist)\n",
    "    \n",
    "    add_nodes(scipy_tree, tree)\n",
    "    add_labels(tree['children'][0])\n",
    "    \n",
    "    with open(CORPUS_NAME + '-cluster-original.json', 'w') as clustering:\n",
    "        dump(tree, clustering, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
