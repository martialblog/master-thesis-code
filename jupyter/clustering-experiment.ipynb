{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn.decomposition\n",
    "import sklearn.cluster\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import pandas\n",
    "import random\n",
    "import os\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = '../../histnorm/datasets/historical/german/german-anselm.test.txt'\n",
    "ENCODING = 'utf-8'\n",
    "CORPUS_NAME = 'german-anselm'\n",
    "\n",
    "# Loading input file, which has the original and modernised token in each line separated by a \\t\n",
    "with open(INPUT, 'r', encoding=ENCODING) as infile:\n",
    "    tokens = [line.strip().split('\\t') for line in infile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the original and modernised tokens and types\n",
    "tokens_original = [token[0] for token in tokens[:5000]]\n",
    "tokens_modernised = [token[1] for token in tokens[:5000]]\n",
    "types_original = list(set(tokens_original))\n",
    "types_modernised = list(set(tokens_modernised))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levenshtein Distance\n",
    "def levenshtein(string1, string2):\n",
    "    if string1 == string2:\n",
    "        return 0\n",
    "\n",
    "    if not string2:\n",
    "        return len(string1)\n",
    "    if not string1:\n",
    "        return len(string2)\n",
    "\n",
    "    rows = len(string1) + 1\n",
    "    cols = len(string2) + 1\n",
    "    dist = [[0 for c in range(cols)] for r in range(rows)]\n",
    "\n",
    "    for j in range(1, rows):\n",
    "        dist[j][0] = j\n",
    "    for i in range(1, cols):\n",
    "        dist[0][i] = i\n",
    "\n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            cost = 1\n",
    "            if string1[row - 1] == string2[col - 1]:\n",
    "                cost = 0\n",
    "            dist[row][col] = min(dist[row - 1][col] + 1, dist[row][col - 1] + 1, dist[row - 1][col - 1] + cost)\n",
    "\n",
    "    # Enable for Debugging\n",
    "    # print('\\n'.join([''.join(['{:4}'.format(elem) for elem in row]) for row in dist]))\n",
    "    return dist[row][col]\n",
    "\n",
    "assert levenshtein('', '') == 0\n",
    "assert levenshtein('foobar', 'foobar') == 0\n",
    "assert levenshtein('foobar', 'foubar') == 1\n",
    "assert levenshtein('foobar', 'fuubar') == 2\n",
    "assert levenshtein('foobar', 'fuuar') == 3\n",
    "assert levenshtein('foobar', '') == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaro Similarily\n",
    "def jaro(string1, string2):\n",
    "\n",
    "    length1 = len(string1)\n",
    "    length2 = len(string2)\n",
    "   \n",
    "    if length1 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    if string1 == string2:\n",
    "        return 1.0   \n",
    "\n",
    "    match_bound = max(length1, length2) // 2 - 1\n",
    "\n",
    "    matches = 0  \n",
    "    transpositions = 0\n",
    "\n",
    "    flagged_1 = [] \n",
    "    flagged_2 = []\n",
    "\n",
    "    for i in range(length1):\n",
    "        upperbound = min(i + match_bound, length2 - 1)\n",
    "        lowerbound = max(0, i - match_bound)\n",
    "        for j in range(lowerbound, upperbound + 1):\n",
    "            if string1[i] == string2[j] and j not in flagged_2:\n",
    "                matches += 1\n",
    "                flagged_1.append(i)\n",
    "                flagged_2.append(j)\n",
    "                break\n",
    "\n",
    "    flagged_2.sort()\n",
    "\n",
    "    for i, j in zip(flagged_1, flagged_2):\n",
    "        if string1[i] != string2[j]:\n",
    "            transpositions += 1\n",
    "\n",
    "    if matches == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return (1/3 * ( matches / length1 + matches / length2 + (matches - transpositions // 2) / matches))\n",
    "\n",
    "assert jaro('', '') == 0.0\n",
    "assert jaro('foobar', '') == 0.0\n",
    "assert jaro('foobar', 'foobar') == 1.0\n",
    "assert jaro('foobar', 'barfoo') == 0.4444444444444444\n",
    "assert jaro('duane', 'dwayne') == 0.8222222222222222\n",
    "assert jaro('hans', 'gruber') == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBM (LCS-Levenshtein Normalized)\n",
    "\n",
    "# Contractor, D., Faruquie, T. A., & Subramaniam, L. V. (2010, August). \n",
    "# Unsupervised cleansing of noisy text. \n",
    "# In Proceedings of the 23rd International Conference on Computational Linguistics:\n",
    "# Posters (pp. 189-196). Association for Computational Linguistics.\n",
    "\n",
    "from itertools import groupby\n",
    "\n",
    "# Longest Common Substring\n",
    "def longest_common_string(string1, string2):\n",
    "    if string1 == string2:\n",
    "        return len(string1)\n",
    "\n",
    "    if not string1 or not string2:\n",
    "        return 0\n",
    "    \n",
    "    rows = len(string1) + 1\n",
    "    cols = len(string2) + 1\n",
    "    table = [[0 for c in range(cols)] for r in range(rows)]\n",
    "\n",
    "    longest = 0\n",
    "    for col in range(cols):\n",
    "        for row in range(rows):\n",
    "            if col == 0 and row == 0:\n",
    "                table[row][col] = 0\n",
    "            if string1[row - 1] == string2[col - 1]:\n",
    "                table[row][col] = table[row - 1][col - 1] + 1\n",
    "                longest = max(longest, table[row][col])\n",
    "            else:\n",
    "                table[row][col] = 0\n",
    "    \n",
    "    return longest\n",
    "\n",
    "assert longest_common_string('', '') == 0\n",
    "assert longest_common_string('foobar', '') == 0\n",
    "assert longest_common_string('foobar', 'foobar') == 6\n",
    "assert longest_common_string('foobar', 'foo') == 3\n",
    "assert longest_common_string('foobar', 'f') == 1\n",
    "\n",
    "\n",
    "def lcs_ratio(string1, string2):\n",
    "    if not string1 or not string2:\n",
    "        return 0.0\n",
    "    ratio = longest_common_string(string1, string2) / len(string1)\n",
    "    return ratio\n",
    "\n",
    "assert lcs_ratio('', '') == 0.0\n",
    "assert lcs_ratio('foo', '') == 0.0\n",
    "assert lcs_ratio('foobar', 'foobar') == 1.0\n",
    "assert lcs_ratio('foo', 'bar') == 0.0\n",
    "assert lcs_ratio('word', 'deoxyribonucleic') == 0.25\n",
    "\n",
    "\n",
    "def consonant_skeleton(string, vowels='aeiouy'):\n",
    "    without_vowels = ''.join([char for char in string if char not in vowels])     \n",
    "    deduplicated_consonants = ''.join(char for char, _ in groupby(without_vowels))\n",
    "    return deduplicated_consonants\n",
    "\n",
    "assert consonant_skeleton('') == ''\n",
    "assert consonant_skeleton('aeio') == ''\n",
    "assert consonant_skeleton('foobar') == 'fbr'\n",
    "assert consonant_skeleton('ffoobbar') == 'fbr'\n",
    "assert consonant_skeleton('barfoobar') == 'brfbr'\n",
    "\n",
    "\n",
    "def ibm_similarity(string1, string2):\n",
    "    similarity = lcs_ratio(string1, string2) / (levenshtein (consonant_skeleton(string1), consonant_skeleton(string2)) + 1)\n",
    "    return similarity\n",
    "\n",
    "assert ibm_similarity('', '') == 0.0\n",
    "assert ibm_similarity('foobar', '') == 0.0\n",
    "assert ibm_similarity('foobar', 'foobar') == 1.0\n",
    "assert ibm_similarity('foo', 'bar') == 0.0\n",
    "assert ibm_similarity('word', 'deoxyribonucleic') == 0.03125\n",
    "assert ibm_similarity('foobar', 'aeiou') == 0.041666666666666664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "types_original_reshaped = np.array(types_original).reshape(-1,1)\n",
    "types_original_pairwise_distance_levenshtein = scipy.spatial.distance.pdist(types_original_reshaped, lambda x,y: levenshtein(str(x[0]),str(y[0])))   \n",
    "types_original_pairwise_distance_jaro = scipy.spatial.distance.pdist(types_original_reshaped, lambda x,y: jaro(str(x[0]),str(y[0])))   \n",
    "types_original_pairwise_distance_ibm = scipy.spatial.distance.pdist(types_original_reshaped, lambda x,y: ibm_similarity(str(x[0]),str(y[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Transforming pairwise distances into a full similarity matrix\n",
    "original_distance_matrix_levenshtein = pandas.DataFrame(scipy.spatial.distance.squareform(types_original_pairwise_distance_levenshtein), index=types_original, columns=types_original)\n",
    "\n",
    "original_distance_matrix_jaro = pandas.DataFrame(scipy.spatial.distance.squareform(types_original_pairwise_distance_jaro), index=types_original, columns=types_original)\n",
    "\n",
    "original_distance_matrix_ibm = pandas.DataFrame(scipy.spatial.distance.squareform(types_original_pairwise_distance_ibm), index=types_original, columns=types_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_n_clusters(model):\n",
    "    print('\\n{} Number of Clusters'.format(model.n_clusters_))\n",
    "    print('---')\n",
    "    \n",
    "def print_random_clusters(model, n=10):\n",
    "    print('\\n{} Random Clusters'.format(n))\n",
    "    print('---')\n",
    "    for cluster_id in random.choices(np.unique(model.labels_), k=10):\n",
    "        cluster = types_original_reshaped[np.nonzero(model.labels_ == cluster_id)]\n",
    "        flatten = [item for sublist in cluster for item in sublist]\n",
    "        print(flatten)\n",
    "\n",
    "def print_largest_clusters(model, n=10):\n",
    "    print('\\n{} Largest Clusters'.format(n))\n",
    "    print('---')\n",
    "\n",
    "    clusters = []\n",
    "    for cluster_id in np.unique(model.labels_):\n",
    "        cluster = types_original_reshaped[np.nonzero(model.labels_ == cluster_id)]\n",
    "        clusters.append([item for sublist in cluster for item in sublist])\n",
    "    clusters.sort(key=len)\n",
    "    \n",
    "    for cl in clusters[-n:]:\n",
    "        print('{}: {}'.format(len(cl), cl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affinity Propagation\n",
    "\n",
    "Damping factor (between 0.5 and 1) is the extent to which the current value is maintained relative to incoming values \n",
    "(weighted 1 - damping). This in order to avoid numerical oscillations when updating these values (messages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damping_factor = 0.9\n",
    "affinity_levenshtein_euclidean = sklearn.cluster.AffinityPropagation(affinity='euclidean', damping=damping_factor, random_state=None).fit(original_distance_matrix_levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damping_factor = 0.9\n",
    "affinity_jaro_euclidean = sklearn.cluster.AffinityPropagation(affinity='euclidean', damping=damping_factor, random_state=None).fit(original_distance_matrix_jaro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damping_factor = 0.9\n",
    "affinity_ibm_euclidean = sklearn.cluster.AffinityPropagation(affinity='euclidean', damping=damping_factor, random_state=None).fit(original_distance_matrix_ibm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLevenshtein')\n",
    "print_random_clusters(affinity_levenshtein_euclidean)\n",
    "print_largest_clusters(affinity_levenshtein_euclidean)\n",
    "\n",
    "print('\\nJaro')\n",
    "print_random_clusters(affinity_jaro_euclidean)\n",
    "print_largest_clusters(affinity_jaro_euclidean)\n",
    "\n",
    "print('\\nIBM')\n",
    "print_random_clusters(affinity_ibm_euclidean)\n",
    "print_largest_clusters(affinity_ibm_euclidean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "\n",
    "eps, The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "\n",
    "min_samples, The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_factor = 0.5\n",
    "min_samples = 5\n",
    "dbscan_levenshtein = sklearn.cluster.DBSCAN(eps=eps_factor,\n",
    "                                            min_samples=min_samples).fit(original_distance_matrix_levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_factor = 0.5\n",
    "min_samples = 5\n",
    "dbscan_jaro = sklearn.cluster.DBSCAN(eps=eps_factor,\n",
    "                                            min_samples=min_samples).fit(original_distance_matrix_jaro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_factor = 0.5\n",
    "min_samples = 5\n",
    "dbscan_ibm = sklearn.cluster.DBSCAN(eps=eps_factor,\n",
    "                                            min_samples=min_samples).fit(original_distance_matrix_ibm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLevenshtein')\n",
    "# print_random_clusters(dbscan_levenshtein)\n",
    "print_largest_clusters(dbscan_levenshtein)\n",
    "\n",
    "print('\\nJaro')\n",
    "# print_random_clusters(dbscan_jaro)\n",
    "print_largest_clusters(dbscan_jaro)\n",
    "\n",
    "print('\\nIBM')\n",
    "# print_random_clusters(dbscan_ibm)\n",
    "print_largest_clusters(dbscan_ibm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative Clustering\n",
    "\n",
    "The linkage distance threshold above which, clusters will not be merged. \n",
    "\n",
    "If not None, n_clusters must be None and compute_full_tree must be True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_method = 'single'\n",
    "distance_threshold = 2\n",
    "agglomerative_single_levenshtein = sklearn.cluster.AgglomerativeClustering(n_clusters=None, \n",
    "                                                                           distance_threshold=distance_threshold, \n",
    "                                                                           affinity='precomputed', \n",
    "                                                                           linkage=linkage_method).fit(original_distance_matrix_levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLevenshtein')\n",
    "print_random_clusters(agglomerative_single_levenshtein)\n",
    "print_largest_clusters(agglomerative_single_levenshtein)\n",
    "print_n_clusters(agglomerative_single_levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_method = 'complete'\n",
    "distance_threshold = 2\n",
    "agglomerative_complete_levenshtein = sklearn.cluster.AgglomerativeClustering(n_clusters=None, \n",
    "                                                                           distance_threshold=distance_threshold, \n",
    "                                                                           affinity='precomputed', \n",
    "                                                                           linkage=linkage_method).fit(original_distance_matrix_levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_method = 'complete'\n",
    "distance_threshold = 0.3\n",
    "agglomerative_complete_jaro = sklearn.cluster.AgglomerativeClustering(n_clusters=None, \n",
    "                                                                           distance_threshold=distance_threshold, \n",
    "                                                                           affinity='precomputed', \n",
    "                                                                           linkage=linkage_method).fit(original_distance_matrix_jaro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_method = 'complete'\n",
    "distance_threshold = 0.3\n",
    "agglomerative_complete_ibm = sklearn.cluster.AgglomerativeClustering(n_clusters=None, \n",
    "                                                                           distance_threshold=distance_threshold, \n",
    "                                                                           affinity='precomputed', \n",
    "                                                                           linkage=linkage_method).fit(original_distance_matrix_ibm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLevenshtein')\n",
    "# print_random_clusters(agglomerative_complete_levenshtein)\n",
    "print_largest_clusters(agglomerative_complete_levenshtein)\n",
    "print_n_clusters(agglomerative_complete_levenshtein)\n",
    "\n",
    "print('\\nJaro')\n",
    "# print_random_clusters(agglomerative_complete_jaro)\n",
    "print_largest_clusters(agglomerative_complete_jaro)\n",
    "print_n_clusters(agglomerative_complete_jaro)\n",
    "\n",
    "print('\\nIBM')\n",
    "# print_random_clusters(agglomerative_complete_ibm)\n",
    "print_largest_clusters(agglomerative_complete_ibm)\n",
    "print_n_clusters(agglomerative_complete_ibm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_method = 'average'\n",
    "distance_threshold = 2\n",
    "agglomerative_average_levenshtein = sklearn.cluster.AgglomerativeClustering(n_clusters=None, \n",
    "                                                                           distance_threshold=distance_threshold, \n",
    "                                                                           affinity='precomputed', \n",
    "                                                                           linkage=linkage_method).fit(original_distance_matrix_levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_method = 'average'\n",
    "distance_threshold = 0.1\n",
    "agglomerative_average_jaro = sklearn.cluster.AgglomerativeClustering(n_clusters=None, \n",
    "                                                                           distance_threshold=distance_threshold, \n",
    "                                                                           affinity='precomputed', \n",
    "                                                                           linkage=linkage_method).fit(original_distance_matrix_jaro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_method = 'average'\n",
    "distance_threshold = 0.1\n",
    "agglomerative_average_ibm = sklearn.cluster.AgglomerativeClustering(n_clusters=None, \n",
    "                                                                           distance_threshold=distance_threshold, \n",
    "                                                                           affinity='precomputed', \n",
    "                                                                           linkage=linkage_method).fit(original_distance_matrix_ibm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLevenshtein')\n",
    "# print_random_clusters(agglomerative_average_levenshtein)\n",
    "print_largest_clusters(agglomerative_average_levenshtein)\n",
    "print_n_clusters(agglomerative_average_levenshtein)\n",
    "\n",
    "print('\\nJaro')\n",
    "# print_random_clusters(agglomerative_average_jaro)\n",
    "print_largest_clusters(agglomerative_average_jaro)\n",
    "print_n_clusters(agglomerative_average_jaro)\n",
    "\n",
    "print('\\nIBM')\n",
    "# print_random_clusters(agglomerative_average_ibm)\n",
    "print_largest_clusters(agglomerative_average_ibm)\n",
    "print_n_clusters(agglomerative_average_ibm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_method = 'ward'\n",
    "distance_threshold = 9\n",
    "agglomerative_ward_levenshtein = sklearn.cluster.AgglomerativeClustering(n_clusters=None, \n",
    "                                                                           distance_threshold=distance_threshold, \n",
    "                                                                           affinity='euclidean', \n",
    "                                                                           linkage=linkage_method).fit(original_distance_matrix_levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_method = 'ward'\n",
    "distance_threshold = 2\n",
    "agglomerative_ward_jaro = sklearn.cluster.AgglomerativeClustering(n_clusters=None, \n",
    "                                                                           distance_threshold=distance_threshold, \n",
    "                                                                           affinity='euclidean', \n",
    "                                                                           linkage=linkage_method).fit(original_distance_matrix_jaro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_method = 'ward'\n",
    "distance_threshold = 1\n",
    "agglomerative_ward_ibm= sklearn.cluster.AgglomerativeClustering(n_clusters=None, \n",
    "                                                                           distance_threshold=distance_threshold, \n",
    "                                                                           affinity='euclidean', \n",
    "                                                                           linkage=linkage_method).fit(original_distance_matrix_ibm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLevenshtein')\n",
    "# print_random_clusters(agglomerative_ward_levenshtein)\n",
    "print_largest_clusters(agglomerative_ward_levenshtein)\n",
    "print_n_clusters(agglomerative_ward_levenshtein)\n",
    "\n",
    "print('\\nJaro')\n",
    "# print_random_clusters(agglomerative_ward_jaro)\n",
    "print_largest_clusters(agglomerative_ward_jaro)\n",
    "print_n_clusters(agglomerative_ward_jaro)\n",
    "\n",
    "print('\\nIBM')\n",
    "# print_random_clusters(agglomerative_ward_ibm)\n",
    "print_largest_clusters(agglomerative_ward_ibm)\n",
    "print_n_clusters(agglomerative_ward_ibm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
