{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spanish P.S. Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os.path\n",
    "import scipy\n",
    "import scipy.spatial\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import pandas\n",
    "import collections\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- Reading in the corpus data\n",
    "- Splitting into historical/original and modernised portion\n",
    "- Creating distance matrix from the historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = '../../histnorm/datasets/historical/spanish/spanish-ps<n>.dev.txt'\n",
    "ENCODING = 'utf-8'\n",
    "CORPUS_NAME = 'spanish-ps'\n",
    "FILTER = ('\"', \"'\", '#', '.', ',', '(', ')', ';', '—', '/')\n",
    "\n",
    "tokens_raw = []\n",
    "\n",
    "# Loading input file, which has the original and modernised token in each line separated by a \\t\n",
    "for n in range(16,20):\n",
    "    inputfile = INPUT.replace('<n>', str(n))\n",
    "    with open(inputfile, 'r', encoding=ENCODING) as infile:\n",
    "        tokens_raw += [line.strip().split('\\t') for line in infile]\n",
    "\n",
    "# Filter out lines with control characters\n",
    "tokens = [token for token in tokens_raw if len(token)>1 and not token[0].startswith(FILTER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the original and modernised tokens and types\n",
    "tokens_original = [token[0].lower() for token in tokens]\n",
    "tokens_modernised = [token[1].lower() for token in tokens]\n",
    "\n",
    "types_original = list(set(tokens_original))\n",
    "types_modernised = list(set(tokens_modernised))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token and hapax count\n",
    "tokens_original_count = collections.Counter(tokens_original)\n",
    "tokens_modernised_count = collections.Counter(tokens_modernised)\n",
    "\n",
    "hapax_original_count = len([val for val in tokens_original_count.values() if val == 1])\n",
    "hapax_modernised_count = len([val for val in tokens_modernised_count.values() if val == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate evaluation clustering from modernised tokens\n",
    "\n",
    "evaluation_cluster = dict()\n",
    "\n",
    "for token in tokens:\n",
    "    hist = token[0].lower()\n",
    "    cont = token[1].lower()\n",
    "    if cont in evaluation_cluster:\n",
    "        evaluation_cluster[cont].append(hist)\n",
    "    else:\n",
    "        evaluation_cluster[cont] = [hist]\n",
    "\n",
    "# Reduce Items to unique Items \n",
    "evaluation_cluster_types = dict()\n",
    "\n",
    "for idx, items in evaluation_cluster.items():\n",
    "    evaluation_cluster_types[idx] = list(set(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levenshtein Distance\n",
    "def levenshtein(string1, string2):\n",
    "    if string1 == string2:\n",
    "        return 0\n",
    "\n",
    "    if not string2:\n",
    "        return len(string1)\n",
    "    if not string1:\n",
    "        return len(string2)\n",
    "\n",
    "    rows = len(string1) + 1\n",
    "    cols = len(string2) + 1\n",
    "    dist = [[0 for c in range(cols)] for r in range(rows)]\n",
    "\n",
    "    for j in range(1, rows):\n",
    "        dist[j][0] = j\n",
    "    for i in range(1, cols):\n",
    "        dist[0][i] = i\n",
    "\n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            cost = 1\n",
    "            if string1[row - 1] == string2[col - 1]:\n",
    "                cost = 0\n",
    "            dist[row][col] = min(dist[row - 1][col] + 1, dist[row][col - 1] + 1, dist[row - 1][col - 1] + cost)\n",
    "\n",
    "    return dist[row][col]\n",
    "\n",
    "assert levenshtein('', '') == 0\n",
    "assert levenshtein('foobar', 'foobar') == 0\n",
    "assert levenshtein('foobar', 'foubar') == 1\n",
    "assert levenshtein('foobar', 'fuubar') == 2\n",
    "assert levenshtein('foobar', 'fuuar') == 3\n",
    "assert levenshtein('foobar', '') == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaro Similarily\n",
    "def jaro(string1, string2):\n",
    "\n",
    "    length1 = len(string1)\n",
    "    length2 = len(string2)\n",
    "   \n",
    "    if length1 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    if string1 == string2:\n",
    "        return 1.0   \n",
    "\n",
    "    match_bound = max(length1, length2) // 2 - 1\n",
    "\n",
    "    matches = 0  \n",
    "    transpositions = 0\n",
    "\n",
    "    flagged_1 = [] \n",
    "    flagged_2 = []\n",
    "\n",
    "    for i in range(length1):\n",
    "        upperbound = min(i + match_bound, length2 - 1)\n",
    "        lowerbound = max(0, i - match_bound)\n",
    "        for j in range(lowerbound, upperbound + 1):\n",
    "            if string1[i] == string2[j] and j not in flagged_2:\n",
    "                matches += 1\n",
    "                flagged_1.append(i)\n",
    "                flagged_2.append(j)\n",
    "                break\n",
    "\n",
    "    flagged_2.sort()\n",
    "\n",
    "    for i, j in zip(flagged_1, flagged_2):\n",
    "        if string1[i] != string2[j]:\n",
    "            transpositions += 1\n",
    "\n",
    "    if matches == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return (1/3 * ( matches / length1 + matches / length2 + (matches - transpositions // 2) / matches))\n",
    "\n",
    "assert jaro('', '') == 0.0\n",
    "assert jaro('foobar', '') == 0.0\n",
    "assert jaro('foobar', 'foobar') == 1.0\n",
    "assert jaro('foobar', 'barfoo') == 0.4444444444444444\n",
    "assert jaro('duane', 'dwayne') == 0.8222222222222222\n",
    "assert jaro('hans', 'gruber') == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBM (LCS-Levenshtein Normalized)\n",
    "\n",
    "# Contractor, D., Faruquie, T. A., & Subramaniam, L. V. (2010, August). \n",
    "# Unsupervised cleansing of noisy text. \n",
    "# In Proceedings of the 23rd International Conference on Computational Linguistics:\n",
    "# Posters (pp. 189-196). Association for Computational Linguistics.\n",
    "\n",
    "from itertools import groupby\n",
    "\n",
    "# Longest Common Substring\n",
    "def longest_common_string(string1, string2):\n",
    "    if string1 == string2:\n",
    "        return len(string1)\n",
    "\n",
    "    if not string1 or not string2:\n",
    "        return 0\n",
    "    \n",
    "    rows = len(string1) + 1\n",
    "    cols = len(string2) + 1\n",
    "    table = [[0 for c in range(cols)] for r in range(rows)]\n",
    "\n",
    "    longest = 0\n",
    "    for col in range(cols):\n",
    "        for row in range(rows):\n",
    "            if col == 0 and row == 0:\n",
    "                table[row][col] = 0\n",
    "            if string1[row - 1] == string2[col - 1]:\n",
    "                table[row][col] = table[row - 1][col - 1] + 1\n",
    "                longest = max(longest, table[row][col])\n",
    "            else:\n",
    "                table[row][col] = 0\n",
    "    \n",
    "    return longest\n",
    "\n",
    "assert longest_common_string('', '') == 0\n",
    "assert longest_common_string('foobar', '') == 0\n",
    "assert longest_common_string('foobar', 'foobar') == 6\n",
    "assert longest_common_string('foobar', 'foo') == 3\n",
    "assert longest_common_string('foobar', 'f') == 1\n",
    "\n",
    "\n",
    "def lcs_ratio(string1, string2):\n",
    "    if not string1 or not string2:\n",
    "        return 0.0\n",
    "    ratio = longest_common_string(string1, string2) / len(string1)\n",
    "    return ratio\n",
    "\n",
    "assert lcs_ratio('', '') == 0.0\n",
    "assert lcs_ratio('foo', '') == 0.0\n",
    "assert lcs_ratio('foobar', 'foobar') == 1.0\n",
    "assert lcs_ratio('foo', 'bar') == 0.0\n",
    "assert lcs_ratio('word', 'deoxyribonucleic') == 0.25\n",
    "\n",
    "\n",
    "def consonant_skeleton(string, vowels='aeiouy'):\n",
    "    without_vowels = ''.join([char for char in string if char not in vowels])     \n",
    "    deduplicated_consonants = ''.join(char for char, _ in groupby(without_vowels))\n",
    "    return deduplicated_consonants\n",
    "\n",
    "assert consonant_skeleton('') == ''\n",
    "assert consonant_skeleton('aeio') == ''\n",
    "assert consonant_skeleton('foobar') == 'fbr'\n",
    "assert consonant_skeleton('ffoobbar') == 'fbr'\n",
    "assert consonant_skeleton('barfoobar') == 'brfbr'\n",
    "\n",
    "\n",
    "def ibm_similarity(string1, string2, vowels='aeiouy'):\n",
    "    similarity = lcs_ratio(string1, string2) / (levenshtein (consonant_skeleton(string1, vowels), consonant_skeleton(string2, vowels)) + 1)\n",
    "    return similarity\n",
    "\n",
    "assert ibm_similarity('', '') == 0.0\n",
    "assert ibm_similarity('foobar', '') == 0.0\n",
    "assert ibm_similarity('foobar', 'foobar') == 1.0\n",
    "assert ibm_similarity('foo', 'bar') == 0.0\n",
    "assert ibm_similarity('word', 'deoxyribonucleic') == 0.03125\n",
    "assert ibm_similarity('foobar', 'aeiou') == 0.041666666666666664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precalculated data from cache\n",
    "if os.path.exists('spanish_types_original_pairwise_distance_levenshtein.pickle'):\n",
    "    types_original_reshaped = np.array(types_original).reshape(-1,1)\n",
    "    types_original_pairwise_distance_levenshtein = pickle.load(open('spanish_types_original_pairwise_distance_levenshtein.pickle', 'rb'))\n",
    "    types_original_pairwise_distance_jaro = pickle.load(open('spanish_types_original_pairwise_distance_jaro.pickle', 'rb'))\n",
    "    types_original_pairwise_distance_ibm = pickle.load(open('spanish_types_original_pairwise_distance_ibm.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Compute the Pairwise Distance for each Similarity Measure\n",
    "# Skip this step if you have cached data, it might take a while\n",
    "\n",
    "ibm_vowels = 'aeoiuyjóáéàòâÿíôêè'\n",
    "\n",
    "types_original_reshaped = np.array(types_original).reshape(-1,1)\n",
    "types_original_pairwise_distance_levenshtein = scipy.spatial.distance.pdist(types_original_reshaped, lambda x,y: levenshtein(str(x[0]),str(y[0])))   \n",
    "types_original_pairwise_distance_jaro = scipy.spatial.distance.pdist(types_original_reshaped, lambda x,y: jaro(str(x[0]),str(y[0])))   \n",
    "types_original_pairwise_distance_ibm = scipy.spatial.distance.pdist(types_original_reshaped, lambda x,y: ibm_similarity(str(x[0]),str(y[0]),ibm_vowels)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store precalculated data in cache\n",
    "pickle.dump(types_original_pairwise_distance_levenshtein, open('spanish_types_original_pairwise_distance_levenshtein.pickle', 'wb'))\n",
    "pickle.dump(types_original_pairwise_distance_jaro, open('spanish_types_original_pairwise_distance_jaro.pickle', 'wb'))\n",
    "pickle.dump(types_original_pairwise_distance_ibm, open('spanish_types_original_pairwise_distance_ibm.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Transform the Pairwise Distance for each Similarity Measure into full similarity matrix\n",
    "\n",
    "original_distance_matrix_levenshtein = pandas.DataFrame(scipy.spatial.distance.squareform(types_original_pairwise_distance_levenshtein), index=types_original, columns=types_original)\n",
    "original_distance_matrix_jaro = pandas.DataFrame(scipy.spatial.distance.squareform(types_original_pairwise_distance_jaro), index=types_original, columns=types_original)\n",
    "original_distance_matrix_ibm = pandas.DataFrame(scipy.spatial.distance.squareform(types_original_pairwise_distance_ibm), index=types_original, columns=types_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_expected_n_clusters(model):\n",
    "    \"\"\"\n",
    "    Number of clusters\n",
    "    \"\"\"\n",
    "    return pandas.DataFrame(\n",
    "        columns=['baseline'],\n",
    "        index=['NoCl'],\n",
    "        data=[len(model.items())]\n",
    "    )\n",
    "    \n",
    "def eval_expected_avg_cluster_size(model):\n",
    "    \"\"\"\n",
    "    Calculate expected average cluster size\n",
    "    \"\"\"\n",
    "\n",
    "    avg_cluster_size = sum([len(val) for val in model.values()]) / len(model.values())\n",
    "\n",
    "    return pandas.DataFrame(\n",
    "        columns=['baseline'],\n",
    "        index=['AvgClSize'],\n",
    "        data=[avg_cluster_size]\n",
    "    )\n",
    "\n",
    "def eval_expected_cluster_similarity_stats(model):\n",
    "    \"\"\"\n",
    "    Calculate inter object similarity for evaluation cluster\n",
    "    \"\"\"\n",
    "    \n",
    "    avg_similarities = []\n",
    "\n",
    "    for cluster in model.values():\n",
    "        reshaped = np.array(cluster).reshape(-1,1)\n",
    "        similarity = scipy.spatial.distance.pdist(reshaped, lambda x,y: jaro(str(x[0]),str(y[0])))\n",
    "        avg_similarities.append(np.mean(similarity))\n",
    "\n",
    "    return pandas.DataFrame(\n",
    "        columns=['baseline'],\n",
    "        index=['ObjSimMean', 'ObjSimMedian', 'ObjSimSTD', 'ObjSimVAR'],\n",
    "        data=[\n",
    "            np.nanmean(avg_similarities),\n",
    "            np.nanmedian(avg_similarities),\n",
    "            np.nanstd(avg_similarities),\n",
    "            np.nanvar(avg_similarities)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def eval_expected_largest_clusters(model, n=10):\n",
    "    \"\"\"\n",
    "    Largest clusters\n",
    "    \"\"\"\n",
    "    clusters = list(model.values())\n",
    "    clusters.sort(key=len)    \n",
    "    \n",
    "    c_length = [len(cl) for cl in clusters[-n:]]\n",
    "    c_tokens = [cl for cl in clusters[-n:]]\n",
    "    \n",
    "    return pandas.DataFrame(\n",
    "        data={\n",
    "            'Length': c_length,\n",
    "            'Tokens': c_tokens\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_n_clusters(model):\n",
    "    \"\"\"\n",
    "    Number of clusters\n",
    "    \"\"\"\n",
    "    n_clusters = 0\n",
    "    try:\n",
    "        # Agglomerative Hierarchical Clustering\n",
    "        n_clusters = model.n_clusters_\n",
    "    except AttributeError:\n",
    "        # Affinity Propagation\n",
    "        n_clusters= len(model.cluster_centers_indices_)\n",
    "    \n",
    "    return pandas.DataFrame(\n",
    "        columns=['actual'],\n",
    "        index=['NoCl'],\n",
    "        data=[n_clusters]\n",
    "    )    \n",
    "\n",
    "def eval_random_clusters(model, n=10):\n",
    "    \"\"\"\n",
    "    Random clusters\n",
    "    \"\"\"\n",
    "    rand_clusters = []\n",
    "    for cluster_id in random.choices(np.unique(model.labels_), k=10):\n",
    "        cluster = types_original_reshaped[np.nonzero(model.labels_ == cluster_id)]\n",
    "        rand_clusters.append([item for sublist in cluster for item in sublist])\n",
    "\n",
    "    return pandas.Series(rand_clusters)\n",
    "\n",
    "def eval_largest_clusters(model, n=10):\n",
    "    \"\"\"\n",
    "    Largest clusters\n",
    "    \"\"\"\n",
    "    clusters = []\n",
    "\n",
    "    for cluster_id in np.unique(model.labels_):\n",
    "        cluster = types_original_reshaped[np.nonzero(model.labels_ == cluster_id)]\n",
    "        clusters.append([item for sublist in cluster for item in sublist])\n",
    "    \n",
    "    clusters.sort(key=len)\n",
    "\n",
    "    c_length = [len(cl) for cl in clusters[-n:]]\n",
    "    c_tokens = [cl for cl in clusters[-n:]]\n",
    "    \n",
    "    return pandas.DataFrame(\n",
    "        data={\n",
    "            'Length': c_length,\n",
    "            'Tokens': c_tokens\n",
    "        }\n",
    "    )\n",
    "\n",
    "def eval_cluster_similarity_stats(model):\n",
    "    \"\"\"\n",
    "    Calculate inter object similarity\n",
    "    \"\"\"\n",
    "\n",
    "    avg_similarities = []\n",
    "    for cluster_id in np.unique(model.labels_):\n",
    "        cluster = types_original_reshaped[np.nonzero(model.labels_ == cluster_id)]\n",
    "        similarity = scipy.spatial.distance.pdist(cluster, lambda x,y: jaro(str(x[0]),str(y[0])))\n",
    "        avg_similarities.append(np.mean(similarity))\n",
    "\n",
    "    return pandas.DataFrame(\n",
    "        columns=['actual'],\n",
    "        index=['ObjSimMean', 'ObjSimMedian', 'ObjSimSTD', 'ObjSimVAR'],\n",
    "        data=[\n",
    "            np.nanmean(avg_similarities),\n",
    "            np.nanmedian(avg_similarities),\n",
    "            np.nanstd(avg_similarities),\n",
    "            np.nanvar(avg_similarities)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "def eval_avg_cluster_size(model):\n",
    "    \"\"\"\n",
    "    Calculate average cluster size\n",
    "    \"\"\"\n",
    "\n",
    "    summary = collections.Counter(model.labels_)\n",
    "    avg_cluster_size = sum(summary.values()) / len(summary.items())\n",
    "    \n",
    "    return pandas.DataFrame(\n",
    "        columns=['actual'],\n",
    "        index=['AvgClSize'],\n",
    "        data=[avg_cluster_size]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate various values from manual clustering, for evaluation \n",
    "expected_n_clusters = eval_expected_n_clusters(evaluation_cluster_types)\n",
    "expected_largest_clusters = eval_expected_largest_clusters(evaluation_cluster_types)\n",
    "expected_avg_cluster_size = eval_expected_avg_cluster_size(evaluation_cluster_types)\n",
    "expected_stats = eval_expected_cluster_similarity_stats(evaluation_cluster_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_largest_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity Propagation Clustering\n",
    "\n",
    "- Damping factor (between 0.5 and 1) is the extent to which the current value is maintained relative to incoming values (weighted 1 - damping). \n",
    "- This in order to avoid numerical oscillations when updating these values (messages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parameters\n",
    "damping_factor = 0.9\n",
    "\n",
    "# Calculation\n",
    "apc_levenshtein_euclidean = sklearn.cluster.AffinityPropagation(\n",
    "    affinity='euclidean', \n",
    "    damping=damping_factor, \n",
    "    random_state=None).fit(original_distance_matrix_levenshtein)\n",
    "\n",
    "eval_largest_clusters(apc_levenshtein_euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc_levenshtein_largest_clusters = eval_largest_clusters(apc_levenshtein_euclidean)\n",
    "\n",
    "apc_levenshtein_n_clusters = eval_n_clusters(apc_levenshtein_euclidean)\n",
    "apc_levenshtein_n_clusters = apc_levenshtein_n_clusters.rename(columns={'actual': 'apc_levenshtein'})\n",
    "\n",
    "apc_levenshtein_avg_cluster_size = eval_avg_cluster_size(apc_levenshtein_euclidean)\n",
    "apc_levenshtein_avg_cluster_size = apc_levenshtein_avg_cluster_size.rename(columns={'actual': 'apc_levenshtein'})\n",
    "\n",
    "apc_levenshtein_stats = eval_cluster_similarity_stats(apc_levenshtein_euclidean)\n",
    "apc_levenshtein_stats = apc_levenshtein_stats.rename(columns={'actual': 'apc_levenshtein'})\n",
    "\n",
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, apc_levenshtein_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, apc_levenshtein_n_clusters], axis=1), \n",
    "    pandas.concat([expected_avg_cluster_size, apc_levenshtein_avg_cluster_size], axis=1)\n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parameters\n",
    "damping_factor = 0.9\n",
    "\n",
    "# Calculation\n",
    "apc_jaro_euclidean = sklearn.cluster.AffinityPropagation(\n",
    "    affinity='euclidean',\n",
    "    damping=damping_factor, \n",
    "    random_state=None).fit(original_distance_matrix_jaro)\n",
    "\n",
    "eval_largest_clusters(apc_jaro_euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc_jaro_largest_clusters = eval_largest_clusters(apc_jaro_euclidean)\n",
    "\n",
    "apc_jaro_n_clusters = eval_n_clusters(apc_jaro_euclidean)\n",
    "apc_jaro_n_clusters = apc_jaro_n_clusters.rename(columns={'actual': 'apc_jaro'})\n",
    "\n",
    "apc_jaro_avg_cluster_size = eval_avg_cluster_size(apc_jaro_euclidean)\n",
    "apc_jaro_avg_cluster_size = apc_jaro_avg_cluster_size.rename(columns={'actual': 'apc_jaro'})\n",
    "\n",
    "apc_jaro_stats = eval_cluster_similarity_stats(apc_jaro_euclidean)\n",
    "apc_jaro_stats = apc_jaro_stats.rename(columns={'actual': 'apc_jaro'})\n",
    "\n",
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, apc_jaro_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, apc_jaro_n_clusters], axis=1),\n",
    "    pandas.concat([expected_avg_cluster_size, apc_jaro_avg_cluster_size], axis=1)\n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parameters\n",
    "damping_factor = 0.9\n",
    "\n",
    "# Calculation\n",
    "apc_ibm_euclidean = sklearn.cluster.AffinityPropagation(\n",
    "    affinity='euclidean',\n",
    "    damping=damping_factor, \n",
    "    random_state=None).fit(original_distance_matrix_ibm)\n",
    "\n",
    "eval_largest_clusters(apc_ibm_euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc_ibm_largest_clusters = eval_largest_clusters(apc_ibm_euclidean)\n",
    "\n",
    "apc_ibm_n_clusters = eval_n_clusters(apc_ibm_euclidean)\n",
    "apc_ibm_n_clusters = apc_ibm_n_clusters.rename(columns={'actual': 'apc_ibm'})\n",
    "\n",
    "apc_ibm_avg_cluster_size = eval_avg_cluster_size(apc_ibm_euclidean)\n",
    "apc_ibm_avg_cluster_size = apc_ibm_avg_cluster_size.rename(columns={'actual': 'apc_ibm'})\n",
    "\n",
    "apc_ibm_stats = eval_cluster_similarity_stats(apc_ibm_euclidean)\n",
    "apc_ibm_stats = apc_ibm_stats.rename(columns={'actual': 'apc_ibm'})\n",
    "\n",
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, apc_ibm_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, apc_ibm_n_clusters], axis=1),\n",
    "    pandas.concat([expected_avg_cluster_size, apc_ibm_avg_cluster_size], axis=1)\n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APC Evaluation\n",
    "\n",
    "- Silhouette Score\n",
    "- Number of Clusters\n",
    "- Largest Clusters\n",
    "- Average Cluster Size\n",
    "- Average Cluster Object Jaro Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_levenshtein_euclidean = sklearn.metrics.silhouette_score(original_distance_matrix_levenshtein, apc_levenshtein_euclidean.labels_, metric='euclidean')\n",
    "sc_jaro_euclidean = sklearn.metrics.silhouette_score(original_distance_matrix_jaro, apc_jaro_euclidean.labels_, metric='euclidean')\n",
    "sc_ibm_euclidean = sklearn.metrics.silhouette_score(original_distance_matrix_ibm, apc_ibm_euclidean.labels_, metric='euclidean')\n",
    "\n",
    "pandas.DataFrame(index=['silhouette_score'], data={\n",
    "    'Levenshtein': [sc_levenshtein_euclidean],\n",
    "    'Jaro': [sc_jaro_euclidean],\n",
    "    'IBM': [sc_ibm_euclidean]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, apc_levenshtein_stats, apc_jaro_stats,  apc_ibm_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, apc_levenshtein_n_clusters, apc_jaro_n_clusters,  apc_ibm_n_clusters], axis=1),\n",
    "    pandas.concat([expected_avg_cluster_size, apc_levenshtein_avg_cluster_size, apc_jaro_avg_cluster_size,  apc_ibm_avg_cluster_size], axis=1)    \n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Hierarchical Clustering\n",
    "\n",
    "- Linkage distance threshold above which, clusters will not be merged. \n",
    "- If not None, n_clusters must be None and compute_full_tree must be True.\n",
    "- Metric used to compute the linkage. Can be “euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or “precomputed”. \n",
    "- If linkage is “ward”, only “euclidean” is accepted. \n",
    "- If “precomputed”, a distance matrix (instead of a similarity matrix) is needed as input for the fit method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AHC Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parameters\n",
    "linkage_method = 'single'\n",
    "distance_threshold = 25\n",
    "\n",
    "# Calculation\n",
    "ahc_levenshtein_single = sklearn.cluster.AgglomerativeClustering(\n",
    "    n_clusters=None, \n",
    "    distance_threshold=distance_threshold, \n",
    "    affinity='euclidean', \n",
    "    linkage=linkage_method).fit(original_distance_matrix_levenshtein)\n",
    "\n",
    "eval_largest_clusters(ahc_levenshtein_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahc_levenshtein_single_largest_clusters = eval_largest_clusters(ahc_levenshtein_single)\n",
    "\n",
    "ahc_levenshtein_single_n_clusters = eval_n_clusters(ahc_levenshtein_single)\n",
    "ahc_levenshtein_single_n_clusters = ahc_levenshtein_single_n_clusters.rename(columns={'actual': 'ahc_levenshtein_single'})\n",
    "\n",
    "ahc_levenshtein_single_avg_cluster_size = eval_avg_cluster_size(ahc_levenshtein_single)\n",
    "ahc_levenshtein_single_avg_cluster_size = ahc_levenshtein_single_avg_cluster_size.rename(columns={'actual': 'ahc_levenshtein_single'})\n",
    "\n",
    "ahc_levenshtein_single_stats = eval_cluster_similarity_stats(ahc_levenshtein_single)\n",
    "ahc_levenshtein_single_stats = ahc_levenshtein_single_stats.rename(columns={'actual': 'ahc_levenshtein_single'})\n",
    "\n",
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, ahc_levenshtein_single_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, ahc_levenshtein_single_n_clusters], axis=1),\n",
    "    pandas.concat([expected_avg_cluster_size, ahc_levenshtein_single_avg_cluster_size], axis=1)\n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parameters\n",
    "linkage_method = 'complete'\n",
    "distance_threshold = 27\n",
    "\n",
    "# Calculation\n",
    "ahc_levenshtein_complete = sklearn.cluster.AgglomerativeClustering(\n",
    "    n_clusters=None, \n",
    "    distance_threshold=distance_threshold, \n",
    "    affinity='euclidean', \n",
    "    linkage=linkage_method).fit(original_distance_matrix_levenshtein)\n",
    "\n",
    "eval_largest_clusters(ahc_levenshtein_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahc_levenshtein_complete_largest_clusters = eval_largest_clusters(ahc_levenshtein_complete)\n",
    "\n",
    "ahc_levenshtein_complete_n_clusters = eval_n_clusters(ahc_levenshtein_complete)\n",
    "ahc_levenshtein_complete_n_clusters = ahc_levenshtein_complete_n_clusters.rename(columns={'actual': 'ahc_levenshtein_complete'})\n",
    "\n",
    "ahc_levenshtein_complete_avg_cluster_size = eval_avg_cluster_size(ahc_levenshtein_complete)\n",
    "ahc_levenshtein_complete_avg_cluster_size = ahc_levenshtein_complete_avg_cluster_size.rename(columns={'actual': 'ahc_levenshtein_complete'})\n",
    "\n",
    "ahc_levenshtein_complete_stats = eval_cluster_similarity_stats(ahc_levenshtein_complete)\n",
    "ahc_levenshtein_complete_stats = ahc_levenshtein_complete_stats.rename(columns={'actual': 'ahc_levenshtein_complete'})\n",
    "\n",
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, ahc_levenshtein_complete_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, ahc_levenshtein_complete_n_clusters], axis=1),\n",
    "    pandas.concat([expected_avg_cluster_size, ahc_levenshtein_complete_avg_cluster_size], axis=1)\n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parameters\n",
    "linkage_method = 'average'\n",
    "distance_threshold = 25\n",
    "\n",
    "# Calculation\n",
    "ahc_levenshtein_average = sklearn.cluster.AgglomerativeClustering(\n",
    "    n_clusters=None, \n",
    "    distance_threshold=distance_threshold, \n",
    "    affinity='euclidean', \n",
    "    linkage=linkage_method).fit(original_distance_matrix_levenshtein)\n",
    "\n",
    "eval_largest_clusters(ahc_levenshtein_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahc_levenshtein_average_largest_clusters = eval_largest_clusters(ahc_levenshtein_average)\n",
    "\n",
    "ahc_levenshtein_average_n_clusters = eval_n_clusters(ahc_levenshtein_average)\n",
    "ahc_levenshtein_average_n_clusters = ahc_levenshtein_average_n_clusters.rename(columns={'actual': 'ahc_levenshtein_average'})\n",
    "\n",
    "ahc_levenshtein_average_avg_cluster_size = eval_avg_cluster_size(ahc_levenshtein_average)\n",
    "ahc_levenshtein_average_avg_cluster_size = ahc_levenshtein_average_avg_cluster_size.rename(columns={'actual': 'ahc_levenshtein_average'})\n",
    "\n",
    "ahc_levenshtein_average_stats = eval_cluster_similarity_stats(ahc_levenshtein_average)\n",
    "ahc_levenshtein_average_stats = ahc_levenshtein_average_stats.rename(columns={'actual': 'ahc_levenshtein_average'})\n",
    "\n",
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, ahc_levenshtein_average_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, ahc_levenshtein_average_n_clusters], axis=1),\n",
    "    pandas.concat([expected_avg_cluster_size, ahc_levenshtein_average_avg_cluster_size], axis=1)\n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parameters\n",
    "linkage_method = 'ward'\n",
    "distance_threshold = 25\n",
    "\n",
    "# Calculation\n",
    "ahc_levenshtein_ward = sklearn.cluster.AgglomerativeClustering(\n",
    "    n_clusters=None, \n",
    "    distance_threshold=distance_threshold, \n",
    "    affinity='euclidean', \n",
    "    linkage=linkage_method).fit(original_distance_matrix_levenshtein)\n",
    "\n",
    "eval_largest_clusters(ahc_levenshtein_ward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahc_levenshtein_ward_largest_clusters = eval_largest_clusters(ahc_levenshtein_ward)\n",
    "\n",
    "ahc_levenshtein_ward_n_clusters = eval_n_clusters(ahc_levenshtein_ward)\n",
    "ahc_levenshtein_ward_n_clusters = ahc_levenshtein_ward_n_clusters.rename(columns={'actual': 'ahc_levenshtein_ward'})\n",
    "\n",
    "ahc_levenshtein_ward_avg_cluster_size = eval_avg_cluster_size(ahc_levenshtein_ward)\n",
    "ahc_levenshtein_ward_avg_cluster_size = ahc_levenshtein_ward_avg_cluster_size.rename(columns={'actual': 'ahc_levenshtein_ward'})\n",
    "\n",
    "ahc_levenshtein_ward_stats = eval_cluster_similarity_stats(ahc_levenshtein_ward)\n",
    "ahc_levenshtein_ward_stats = ahc_levenshtein_ward_stats.rename(columns={'actual': 'ahc_levenshtein_ward'})\n",
    "\n",
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, ahc_levenshtein_ward_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, ahc_levenshtein_ward_n_clusters], axis=1),\n",
    "    pandas.concat([expected_avg_cluster_size, ahc_levenshtein_ward_avg_cluster_size], axis=1)\n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AHC Levenshtein Evaluation\n",
    "\n",
    "- Number of Clusters\n",
    "- Largest Clusters\n",
    "- Average Cluster Size (Length of elemets)\n",
    "- Average Cluster Object Jaro Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_levenshtein_single = sklearn.metrics.silhouette_score(original_distance_matrix_levenshtein, ahc_levenshtein_single.labels_, metric='euclidean')\n",
    "sc_levenshtein_complete = sklearn.metrics.silhouette_score(original_distance_matrix_levenshtein, ahc_levenshtein_complete.labels_, metric='euclidean')\n",
    "sc_levenshtein_average = sklearn.metrics.silhouette_score(original_distance_matrix_levenshtein, ahc_levenshtein_average.labels_, metric='euclidean')\n",
    "sc_levenshtein_ward = sklearn.metrics.silhouette_score(original_distance_matrix_levenshtein, ahc_levenshtein_ward.labels_, metric='euclidean')\n",
    "\n",
    "pandas.DataFrame(index=['silhouette_score'], data={\n",
    "    'AHC (single)': [sc_levenshtein_single],\n",
    "    'AHC (complete)': [sc_levenshtein_complete],\n",
    "    'AHC (average)': [sc_levenshtein_average],\n",
    "    'AHC (ward)': [sc_levenshtein_ward]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, ahc_levenshtein_single_stats, ahc_levenshtein_complete_stats, ahc_levenshtein_average_stats, ahc_levenshtein_ward_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, ahc_levenshtein_single_n_clusters, ahc_levenshtein_complete_n_clusters, ahc_levenshtein_average_n_clusters, ahc_levenshtein_ward_n_clusters], axis=1),    \n",
    "    pandas.concat([expected_avg_cluster_size, ahc_levenshtein_single_avg_cluster_size, ahc_levenshtein_complete_avg_cluster_size, ahc_levenshtein_average_avg_cluster_size, ahc_levenshtein_ward_avg_cluster_size], axis=1)    \n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AHC Jaro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parameters\n",
    "linkage_method = 'single'\n",
    "distance_threshold = 5.0\n",
    "\n",
    "# Calculation\n",
    "ahc_jaro_single = sklearn.cluster.AgglomerativeClustering(\n",
    "    n_clusters=None, \n",
    "    distance_threshold=distance_threshold, \n",
    "    affinity='euclidean', \n",
    "    linkage=linkage_method).fit(original_distance_matrix_jaro)\n",
    "\n",
    "eval_largest_clusters(ahc_jaro_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahc_jaro_single_largest_clusters = eval_largest_clusters(ahc_jaro_single)\n",
    "\n",
    "ahc_jaro_single_n_clusters = eval_n_clusters(ahc_jaro_single)\n",
    "ahc_jaro_single_n_clusters = ahc_jaro_single_n_clusters.rename(columns={'actual': 'ahc_jaro_single'})\n",
    "\n",
    "ahc_jaro_single_avg_cluster_size = eval_avg_cluster_size(ahc_jaro_single)\n",
    "ahc_jaro_single_avg_cluster_size = ahc_jaro_single_avg_cluster_size.rename(columns={'actual': 'ahc_jaro_single'})\n",
    "\n",
    "ahc_jaro_single_stats = eval_cluster_similarity_stats(ahc_jaro_single)\n",
    "ahc_jaro_single_stats = ahc_jaro_single_stats.rename(columns={'actual': 'ahc_jaro_single'})\n",
    "\n",
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, ahc_jaro_single_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, ahc_jaro_single_n_clusters], axis=1),\n",
    "    pandas.concat([expected_avg_cluster_size, ahc_jaro_single_avg_cluster_size], axis=1)\n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parameters\n",
    "linkage_method = 'complete'\n",
    "distance_threshold = 5.5\n",
    "\n",
    "# Calculation\n",
    "ahc_jaro_complete = sklearn.cluster.AgglomerativeClustering(\n",
    "    n_clusters=None, \n",
    "    distance_threshold=distance_threshold, \n",
    "    affinity='euclidean', \n",
    "    linkage=linkage_method).fit(original_distance_matrix_jaro)\n",
    "\n",
    "eval_largest_clusters(ahc_jaro_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahc_jaro_complete_largest_clusters = eval_largest_clusters(ahc_jaro_complete)\n",
    "\n",
    "ahc_jaro_complete_n_clusters = eval_n_clusters(ahc_jaro_complete)\n",
    "ahc_jaro_complete_n_clusters = ahc_jaro_complete_n_clusters.rename(columns={'actual': 'ahc_jaro_complete'})\n",
    "\n",
    "ahc_jaro_complete_avg_cluster_size = eval_avg_cluster_size(ahc_jaro_complete)\n",
    "ahc_jaro_complete_avg_cluster_size = ahc_jaro_complete_avg_cluster_size.rename(columns={'actual': 'ahc_jaro_complete'})\n",
    "\n",
    "ahc_jaro_complete_stats = eval_cluster_similarity_stats(ahc_jaro_complete)\n",
    "ahc_jaro_complete_stats = ahc_jaro_complete_stats.rename(columns={'actual': 'ahc_jaro_complete'})\n",
    "\n",
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, ahc_jaro_complete_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, ahc_jaro_complete_n_clusters], axis=1),\n",
    "    pandas.concat([expected_avg_cluster_size, ahc_jaro_complete_avg_cluster_size], axis=1)\n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parameters\n",
    "linkage_method = 'average'\n",
    "distance_threshold = 5.5\n",
    "\n",
    "# Calculation\n",
    "ahc_jaro_average = sklearn.cluster.AgglomerativeClustering(\n",
    "    n_clusters=None, \n",
    "    distance_threshold=distance_threshold, \n",
    "    affinity='euclidean', \n",
    "    linkage=linkage_method).fit(original_distance_matrix_jaro)\n",
    "\n",
    "eval_largest_clusters(ahc_jaro_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahc_jaro_average_largest_clusters = eval_largest_clusters(ahc_jaro_average)\n",
    "\n",
    "ahc_jaro_average_n_clusters = eval_n_clusters(ahc_jaro_average)\n",
    "ahc_jaro_average_n_clusters = ahc_jaro_average_n_clusters.rename(columns={'actual': 'ahc_jaro_average'})\n",
    "\n",
    "ahc_jaro_average_avg_cluster_size = eval_avg_cluster_size(ahc_jaro_average)\n",
    "ahc_jaro_average_avg_cluster_size = ahc_jaro_average_avg_cluster_size.rename(columns={'actual': 'ahc_jaro_average'})\n",
    "\n",
    "ahc_jaro_average_stats = eval_cluster_similarity_stats(ahc_jaro_average)\n",
    "ahc_jaro_average_stats = ahc_jaro_average_stats.rename(columns={'actual': 'ahc_jaro_average'})\n",
    "\n",
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, ahc_jaro_average_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, ahc_jaro_average_n_clusters], axis=1),\n",
    "    pandas.concat([expected_avg_cluster_size, ahc_jaro_average_avg_cluster_size], axis=1)\n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parameters\n",
    "linkage_method = 'ward'\n",
    "distance_threshold = 5.5\n",
    "\n",
    "# Calculation\n",
    "ahc_jaro_ward = sklearn.cluster.AgglomerativeClustering(\n",
    "    n_clusters=None, \n",
    "    distance_threshold=distance_threshold, \n",
    "    affinity='euclidean', \n",
    "    linkage=linkage_method).fit(original_distance_matrix_jaro)\n",
    "\n",
    "eval_largest_clusters(ahc_jaro_ward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahc_jaro_ward_largest_clusters = eval_largest_clusters(ahc_jaro_ward)\n",
    "\n",
    "ahc_jaro_ward_n_clusters = eval_n_clusters(ahc_jaro_ward)\n",
    "ahc_jaro_ward_n_clusters = ahc_jaro_ward_n_clusters.rename(columns={'actual': 'ahc_jaro_ward'})\n",
    "\n",
    "ahc_jaro_ward_avg_cluster_size = eval_avg_cluster_size(ahc_jaro_ward)\n",
    "ahc_jaro_ward_avg_cluster_size = ahc_jaro_ward_avg_cluster_size.rename(columns={'actual': 'ahc_jaro_ward'})\n",
    "\n",
    "ahc_jaro_ward_stats = eval_cluster_similarity_stats(ahc_jaro_ward)\n",
    "ahc_jaro_ward_stats = ahc_jaro_ward_stats.rename(columns={'actual': 'ahc_jaro_ward'})\n",
    "\n",
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, ahc_jaro_ward_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, ahc_jaro_ward_n_clusters], axis=1),\n",
    "    pandas.concat([expected_avg_cluster_size, ahc_jaro_ward_avg_cluster_size], axis=1)\n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AHC Jaro Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_jaro_single = sklearn.metrics.silhouette_score(original_distance_matrix_jaro, ahc_jaro_single.labels_, metric='euclidean')\n",
    "sc_jaro_complete = sklearn.metrics.silhouette_score(original_distance_matrix_jaro, ahc_jaro_complete.labels_, metric='euclidean')\n",
    "sc_jaro_average = sklearn.metrics.silhouette_score(original_distance_matrix_jaro, ahc_jaro_average.labels_, metric='euclidean')\n",
    "sc_jaro_ward = sklearn.metrics.silhouette_score(original_distance_matrix_jaro, ahc_jaro_ward.labels_, metric='euclidean')\n",
    "\n",
    "pandas.DataFrame(index=['silhouette_score'], data={\n",
    "    'AHC (single)': [sc_jaro_single],\n",
    "    'AHC (complete)': [sc_jaro_complete],\n",
    "    'AHC (average)': [sc_jaro_average],\n",
    "    'AHC (ward)': [sc_jaro_ward]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, ahc_jaro_single_stats, ahc_jaro_complete_stats, ahc_jaro_average_stats, ahc_jaro_ward_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, ahc_jaro_single_n_clusters, ahc_jaro_complete_n_clusters, ahc_jaro_average_n_clusters, ahc_jaro_ward_n_clusters], axis=1),    \n",
    "    pandas.concat([expected_avg_cluster_size, ahc_jaro_single_avg_cluster_size, ahc_jaro_complete_avg_cluster_size, ahc_jaro_average_avg_cluster_size, ahc_jaro_ward_avg_cluster_size], axis=1)    \n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AHC IBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parameters\n",
    "linkage_method = 'single'\n",
    "distance_threshold = 0.22\n",
    "\n",
    "# Calculation\n",
    "ahc_ibm_single = sklearn.cluster.AgglomerativeClustering(\n",
    "    n_clusters=None, \n",
    "    distance_threshold=distance_threshold, \n",
    "    affinity='euclidean', \n",
    "    linkage=linkage_method).fit(original_distance_matrix_ibm)\n",
    "\n",
    "eval_largest_clusters(ahc_ibm_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahc_ibm_single_largest_clusters = eval_largest_clusters(ahc_ibm_single)\n",
    "\n",
    "ahc_ibm_single_n_clusters = eval_n_clusters(ahc_ibm_single)\n",
    "ahc_ibm_single_n_clusters = ahc_ibm_single_n_clusters.rename(columns={'actual': 'ahc_ibm_single'})\n",
    "\n",
    "ahc_ibm_single_avg_cluster_size = eval_avg_cluster_size(ahc_ibm_single)\n",
    "ahc_ibm_single_avg_cluster_size = ahc_ibm_single_avg_cluster_size.rename(columns={'actual': 'ahc_ibm_single'})\n",
    "\n",
    "ahc_ibm_single_stats = eval_cluster_similarity_stats(ahc_ibm_single)\n",
    "ahc_ibm_single_stats = ahc_ibm_single_stats.rename(columns={'actual': 'ahc_ibm_single'})\n",
    "\n",
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, ahc_ibm_single_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, ahc_ibm_single_n_clusters], axis=1),\n",
    "    pandas.concat([expected_avg_cluster_size, ahc_ibm_single_avg_cluster_size], axis=1)\n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parameters\n",
    "linkage_method = 'complete'\n",
    "distance_threshold = 0.3\n",
    "\n",
    "# Calculation\n",
    "ahc_ibm_complete = sklearn.cluster.AgglomerativeClustering(\n",
    "    n_clusters=None, \n",
    "    distance_threshold=distance_threshold, \n",
    "    affinity='euclidean', \n",
    "    linkage=linkage_method).fit(original_distance_matrix_ibm)\n",
    "\n",
    "eval_largest_clusters(ahc_ibm_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahc_ibm_complete_largest_clusters = eval_largest_clusters(ahc_ibm_complete)\n",
    "\n",
    "ahc_ibm_complete_n_clusters = eval_n_clusters(ahc_ibm_complete)\n",
    "ahc_ibm_complete_n_clusters = ahc_ibm_complete_n_clusters.rename(columns={'actual': 'ahc_ibm_complete'})\n",
    "\n",
    "ahc_ibm_complete_avg_cluster_size = eval_avg_cluster_size(ahc_ibm_complete)\n",
    "ahc_ibm_complete_avg_cluster_size = ahc_ibm_complete_avg_cluster_size.rename(columns={'actual': 'ahc_ibm_complete'})\n",
    "\n",
    "ahc_ibm_complete_stats = eval_cluster_similarity_stats(ahc_ibm_complete)\n",
    "ahc_ibm_complete_stats = ahc_ibm_complete_stats.rename(columns={'actual': 'ahc_ibm_complete'})\n",
    "\n",
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, ahc_ibm_complete_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, ahc_ibm_complete_n_clusters], axis=1),\n",
    "    pandas.concat([expected_avg_cluster_size, ahc_ibm_complete_avg_cluster_size], axis=1)\n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parameters\n",
    "linkage_method = 'average'\n",
    "distance_threshold = 0.8\n",
    "\n",
    "# Calculation\n",
    "ahc_ibm_average = sklearn.cluster.AgglomerativeClustering(\n",
    "    n_clusters=None, \n",
    "    distance_threshold=distance_threshold, \n",
    "    affinity='euclidean', \n",
    "    linkage=linkage_method).fit(original_distance_matrix_ibm)\n",
    "\n",
    "eval_largest_clusters(ahc_ibm_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahc_ibm_average_largest_clusters = eval_largest_clusters(ahc_ibm_average)\n",
    "\n",
    "ahc_ibm_average_n_clusters = eval_n_clusters(ahc_ibm_average)\n",
    "ahc_ibm_average_n_clusters = ahc_ibm_average_n_clusters.rename(columns={'actual': 'ahc_ibm_average'})\n",
    "\n",
    "ahc_ibm_average_avg_cluster_size = eval_avg_cluster_size(ahc_ibm_average)\n",
    "ahc_ibm_average_avg_cluster_size = ahc_ibm_average_avg_cluster_size.rename(columns={'actual': 'ahc_ibm_average'})\n",
    "\n",
    "ahc_ibm_average_stats = eval_cluster_similarity_stats(ahc_ibm_average)\n",
    "ahc_ibm_average_stats = ahc_ibm_average_stats.rename(columns={'actual': 'ahc_ibm_average'})\n",
    "\n",
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, ahc_jaro_average_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, ahc_jaro_average_n_clusters], axis=1),\n",
    "    pandas.concat([expected_avg_cluster_size, ahc_jaro_average_avg_cluster_size], axis=1)\n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parameters\n",
    "linkage_method = 'ward'\n",
    "distance_threshold = 0.3\n",
    "\n",
    "# Calculation\n",
    "ahc_ibm_ward = sklearn.cluster.AgglomerativeClustering(\n",
    "    n_clusters=None, \n",
    "    distance_threshold=distance_threshold, \n",
    "    affinity='euclidean', \n",
    "    linkage=linkage_method).fit(original_distance_matrix_ibm)\n",
    "\n",
    "eval_largest_clusters(ahc_ibm_ward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahc_ibm_ward_largest_clusters = eval_largest_clusters(ahc_ibm_ward)\n",
    "\n",
    "ahc_ibm_ward_n_clusters = eval_n_clusters(ahc_ibm_ward)\n",
    "ahc_ibm_ward_n_clusters = ahc_ibm_ward_n_clusters.rename(columns={'actual': 'ahc_ibm_ward'})\n",
    "\n",
    "ahc_ibm_ward_avg_cluster_size = eval_avg_cluster_size(ahc_ibm_ward)\n",
    "ahc_ibm_ward_avg_cluster_size = ahc_ibm_ward_avg_cluster_size.rename(columns={'actual': 'ahc_ibm_ward'})\n",
    "\n",
    "ahc_ibm_ward_stats = eval_cluster_similarity_stats(ahc_ibm_ward)\n",
    "ahc_ibm_ward_stats = ahc_ibm_ward_stats.rename(columns={'actual': 'ahc_ibm_ward'})\n",
    "\n",
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, ahc_ibm_ward_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, ahc_ibm_ward_n_clusters], axis=1),\n",
    "    pandas.concat([expected_avg_cluster_size, ahc_ibm_ward_avg_cluster_size], axis=1)\n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AHC IBM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_ibm_single = sklearn.metrics.silhouette_score(original_distance_matrix_ibm, ahc_ibm_single.labels_, metric='euclidean')\n",
    "sc_ibm_complete = sklearn.metrics.silhouette_score(original_distance_matrix_ibm, ahc_ibm_complete.labels_, metric='euclidean')\n",
    "sc_ibm_average = sklearn.metrics.silhouette_score(original_distance_matrix_ibm, ahc_ibm_average.labels_, metric='euclidean')\n",
    "sc_ibm_ward = sklearn.metrics.silhouette_score(original_distance_matrix_ibm, ahc_ibm_ward.labels_, metric='euclidean')\n",
    "\n",
    "pandas.DataFrame(index=['silhouette_score'], data={\n",
    "    'AHC (single)': [sc_ibm_single],\n",
    "    'AHC (complete)': [sc_ibm_complete],\n",
    "    'AHC (average)': [sc_ibm_average],\n",
    "    'AHC (ward)': [sc_ibm_ward]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.concat([\n",
    "    pandas.concat([expected_stats, ahc_ibm_single_stats, ahc_ibm_complete_stats, ahc_ibm_average_stats, ahc_ibm_ward_stats], axis=1),\n",
    "    pandas.concat([expected_n_clusters, ahc_ibm_single_n_clusters, ahc_ibm_complete_n_clusters, ahc_ibm_average_n_clusters, ahc_ibm_ward_n_clusters], axis=1),    \n",
    "    pandas.concat([expected_avg_cluster_size, ahc_ibm_single_avg_cluster_size, ahc_ibm_complete_avg_cluster_size, ahc_ibm_average_avg_cluster_size, ahc_ibm_ward_avg_cluster_size], axis=1)    \n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.concat([\n",
    "    pandas.concat([expected_stats,\n",
    "        ahc_levenshtein_single_stats, \n",
    "        ahc_levenshtein_complete_stats, \n",
    "        ahc_levenshtein_average_stats, \n",
    "        ahc_levenshtein_ward_stats,\n",
    "        ahc_jaro_single_stats, \n",
    "        ahc_jaro_complete_stats, \n",
    "        ahc_jaro_average_stats, \n",
    "        ahc_jaro_ward_stats,\n",
    "        ahc_ibm_single_stats, \n",
    "        ahc_ibm_complete_stats, \n",
    "        ahc_ibm_average_stats, \n",
    "        ahc_ibm_ward_stats\n",
    "    ], axis=1),\n",
    "    pandas.concat([expected_n_clusters, \n",
    "        ahc_levenshtein_single_n_clusters, \n",
    "        ahc_levenshtein_complete_n_clusters, \n",
    "        ahc_levenshtein_average_n_clusters, \n",
    "        ahc_levenshtein_ward_n_clusters,\n",
    "        ahc_jaro_single_n_clusters, \n",
    "        ahc_jaro_complete_n_clusters, \n",
    "        ahc_jaro_average_n_clusters, \n",
    "        ahc_jaro_ward_n_clusters,\n",
    "        ahc_ibm_single_n_clusters, \n",
    "        ahc_ibm_complete_n_clusters, \n",
    "        ahc_ibm_average_n_clusters, \n",
    "        ahc_ibm_ward_n_clusters\n",
    "    ], axis=1),    \n",
    "    pandas.concat([expected_avg_cluster_size,\n",
    "        ahc_levenshtein_single_avg_cluster_size, \n",
    "        ahc_levenshtein_complete_avg_cluster_size, \n",
    "        ahc_levenshtein_average_avg_cluster_size, \n",
    "        ahc_levenshtein_ward_avg_cluster_size,\n",
    "        ahc_jaro_single_avg_cluster_size, \n",
    "        ahc_jaro_complete_avg_cluster_size, \n",
    "        ahc_jaro_average_avg_cluster_size, \n",
    "        ahc_jaro_ward_avg_cluster_size,\n",
    "        ahc_ibm_single_avg_cluster_size, \n",
    "        ahc_ibm_complete_avg_cluster_size, \n",
    "        ahc_ibm_average_avg_cluster_size, \n",
    "        ahc_ibm_ward_avg_cluster_size\n",
    "    ], axis=1)    \n",
    "]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Export\n",
    "\n",
    "Transforms the Clustering into JSON\n",
    "\n",
    "Example Agglomerative Hierarchical output:\n",
    "```\n",
    "{\n",
    "  \"name\": \"Lorem ipsum\",\n",
    "  \"distance\": 5.5,\n",
    "  \"children\": [\n",
    "    {\n",
    "      \"name\": \"dolor\",\n",
    "      \"distance\": 1.5,\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"sit\",\n",
    "          \"distance\": 0.0,\n",
    "          \"children\": []\n",
    "        },\n",
    "...\n",
    "}\n",
    "```\n",
    "\n",
    "Example Affinity Propagation output:\n",
    "\n",
    "```\n",
    "[\n",
    " [\n",
    " \"lorem\",\n",
    " \"impsum\"\n",
    " ],\n",
    " [\n",
    " \"dolor\",\n",
    " \"sit\"\n",
    " ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = dict(enumerate(types_original))\n",
    "\n",
    "def add_nodes(node, parent):\n",
    "    \"\"\"\n",
    "    Recursively build tree as dict\n",
    "    \"\"\"\n",
    "    new_node = dict(node_id=node.id, children=[], distance=node.dist)\n",
    "    parent['children'].append(new_node)\n",
    "    if node.left: add_nodes(node.left, new_node)\n",
    "    if node.right: add_nodes(node.right, new_node)\n",
    "\n",
    "def add_labels(node):\n",
    "    \"\"\"\n",
    "    Recursively add labels to the tree\n",
    "    \"\"\"\n",
    "    is_leaf = len(node['children']) == 0\n",
    "\n",
    "    if is_leaf:\n",
    "        node['name'] = LABELS[node['node_id']]\n",
    "    else:\n",
    "        list(map(add_labels, node['children']))\n",
    "    del node['node_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linkage_matrix(model):\n",
    "    \"\"\"\n",
    "    Creates a scipy-compatible linkage matrix, \n",
    "    so that the hierarchy.to_tree can be used\n",
    "    \"\"\"\n",
    "    \n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([\n",
    "        model.children_, \n",
    "        model.distances_, \n",
    "        counts]).astype(float)\n",
    "\n",
    "    return linkage_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_ahc_model(model, file_name):\n",
    "    \"\"\"\n",
    "    Export a AHC model to JSON\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(file_name):\n",
    "        return\n",
    "    \n",
    "    linkage_matrix = create_linkage_matrix(model)\n",
    "    scipy_tree = scipy.cluster.hierarchy.to_tree(linkage_matrix, rd=False)\n",
    "\n",
    "    tree = dict(name='root', children=[], distance=scipy_tree.dist)\n",
    "\n",
    "    add_nodes(scipy_tree, tree)\n",
    "    add_labels(tree['children'][0])\n",
    "\n",
    "    with open(file_name, 'w') as fp_clustering:\n",
    "        json.dump(tree, fp_clustering, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_apc_model(model, file_name):\n",
    "    \"\"\"\n",
    "    Export a APC model to JSON\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        return\n",
    "    \n",
    "    clusters = []\n",
    "\n",
    "    for cluster_id in model.labels_:\n",
    "        cluster = types_original_reshaped[np.nonzero(model.labels_ == cluster_id)]\n",
    "        clusters.append([item for sublist in cluster for item in sublist])\n",
    "    \n",
    "    # clusters.sort(key=len)\n",
    "    with open(file_name, 'w') as fp_clustering:\n",
    "        json.dump(clusters, fp_clustering, indent=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_apc_model(apc_levenshtein_euclidean, CORPUS_NAME + '-apc_levenshtein.json')\n",
    "export_apc_model(apc_jaro_euclidean, CORPUS_NAME + '-apc_jaro.json')\n",
    "export_apc_model(apc_ibm_euclidean, CORPUS_NAME + '-apc_ibm.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_ahc_model(ahc_levenshtein_single, CORPUS_NAME + '-ahc_levenshtein_single.json')\n",
    "export_ahc_model(ahc_levenshtein_complete, CORPUS_NAME + '-ahc_levenshtein_complete.json')\n",
    "export_ahc_model(ahc_levenshtein_average, CORPUS_NAME + '-ahc_levenshtein_average.json')\n",
    "export_ahc_model(ahc_levenshtein_ward, CORPUS_NAME + '-ahc_levenshtein_ward.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_ahc_model(ahc_jaro_single, CORPUS_NAME + '-ahc_jaro_single.json')\n",
    "export_ahc_model(ahc_jaro_complete, CORPUS_NAME + '-ahc_jaro_complete.json')\n",
    "export_ahc_model(ahc_jaro_average, CORPUS_NAME + '-ahc_jaro_average.json')\n",
    "export_ahc_model(ahc_jaro_ward, CORPUS_NAME + '-ahc_jaro_ward.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_ahc_model(ahc_ibm_single, CORPUS_NAME + '-ahc_ibm_single.json')\n",
    "export_ahc_model(ahc_ibm_complete, CORPUS_NAME + '-ahc_ibm_complete.json')\n",
    "export_ahc_model(ahc_ibm_average, CORPUS_NAME + '-ahc_ibm_average.json')\n",
    "export_ahc_model(ahc_ibm_ward, CORPUS_NAME + '-ahc_ibm_ward.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
